{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Regression\n",
    "\n",
    "This data is a **regression problem**, trying to predict life expectancy.\n",
    "\n",
    "The followings describe the features.\n",
    "\n",
    "- **Country**\n",
    "- **Year**\n",
    "- **Status**: Developed or Developing status\n",
    "- **Life expectancy**: Life Expectancy in age\n",
    "- **Adult Mortality**: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n",
    "- **Infant deaths**: Number of Infant Deaths per 1000 population\n",
    "- **Alcohol**: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n",
    "- **Percentage expenditure**: Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n",
    "- **Hepatitis B**: Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n",
    "- **Measles**: - number of reported cases per 1000 population\n",
    "- **BMI** Average Body Mass Index of entire population\n",
    "- **under-five deaths**: Number of under-five deaths per 1000 population\n",
    "- **Polio**: Polio (Pol3) immunization coverage among 1-year-olds (%)\n",
    "- **Total expenditure**: General government expenditure on health as a percentage of total government expenditure (%)\n",
    "- **Diphtheria**: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n",
    "- **HIV/AIDS**: Deaths per 1000 live births HIV/AIDS (0-4 years)\n",
    "- **GDP**: Gross Domestic Product per capita (in USD)\n",
    "- **Population**: Population of the country\n",
    "- **thinness 1-19 years**: Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n",
    "- **thinness 5-9 years**: Prevalence of thinness among children for Age 5 to 9(%)\n",
    "- **Income composition of resources**: Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n",
    "- **Schooling**: Number of years of Schooling(years)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "np.__version__, pd.__version__, sns.__version__, matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Life_Expectancy_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of your data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical info Hint: look up .describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dtypes of your input data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e747b26a",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "EDA is an essential step to inspect the data, so to better understand nature of the given data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ff88d3",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "\n",
    "Now we would like to rename some of the following column names, so it's easy to write the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns = {'Country':'country', \n",
    "                     'Year':'year', \n",
    "                     'Status':'status', \n",
    "                     'Life expectancy ':'life-exp', \n",
    "                     'Adult Mortality':'adult-mort',\n",
    "                     'infant deaths':'infant-deaths', \n",
    "                     'Alcohol':'alcohol', \n",
    "                     'percentage expenditure':'per-exp', \n",
    "                     'Hepatitis B':'hepa',\n",
    "                     'Measles ':'measles', \n",
    "                     ' BMI ':'bmi', \n",
    "                     'under-five deaths ':'under-five-deaths', \n",
    "                     'Polio':'polio', \n",
    "                     'Total expenditure':'total-exp',\n",
    "                     'Diphtheria ':'dip', \n",
    "                     ' HIV/AIDS':'hiv', \n",
    "                     'GDP':'gdp', \n",
    "                     'Population':'pop',\n",
    "                     ' thinness  1-19 years':'thin1-19', \n",
    "                     ' thinness 5-9 years':'thin5-9',\n",
    "                     'Income composition of resources':'income', \n",
    "                     'Schooling':'school'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the column names changed\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d419fc",
   "metadata": {},
   "source": [
    "### 2.1 Univariate analyis\n",
    "\n",
    "Single variable exploratory data anlaysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf616b09",
   "metadata": {},
   "source": [
    "#### Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd38935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many developing and developed countries there are\n",
    "sns.countplot(data = df, x = 'status')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239be0e2",
   "metadata": {},
   "source": [
    "#### Distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = df, x = 'life-exp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1354e62",
   "metadata": {},
   "source": [
    "### 2.2 Multivariate analysis\n",
    "\n",
    "Multiple variable exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdd851e",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try bar plot on \"Status\"\n",
    "sns.boxplot(x = df[\"status\"], y = df[\"life-exp\"]);\n",
    "plt.ylabel(\"Life Expectancy\")\n",
    "plt.xlabel(\"Status\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67889479",
   "metadata": {},
   "source": [
    "#### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = df['income'], y = df['life-exp'], hue=df['status'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14d97ae",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "\n",
    "Let's use correlation matrix to find strong factors predicting the life expectancy.  It's also for checking whether certain features are too correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30106062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d07a2f",
   "metadata": {},
   "source": [
    "#### Tips: Label encoding\n",
    "\n",
    "Now we would like to change \"Developing\" and \"Developed\" to \"0\" and \"1\", since machine learning algorithms do not understand text.   Also, correlation matrix and other similar computational tools require label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"status\"] = le.fit_transform(df[\"status\"])\n",
    "\n",
    "df[\"status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa068587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can call le.classes_ to know what it maps to\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can try transform\n",
    "le.transform([\"Developed\", \"Developing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b22139",
   "metadata": {},
   "source": [
    "#### Predictive Power Score\n",
    "\n",
    "This is another way to check the predictive power of some feature.  Unlike correlation, `pps` actually obtained from actual prediction.  For more details:\n",
    "    \n",
    "- The score is calculated using only 1 feature trying to predict the target column. This means there are no interaction effects between the scores of various features. Note that this is in contrast to feature importance\n",
    "- The score is calculated on the test sets of a 4-fold crossvalidation (number is adjustable via `ppscore.CV_ITERATIONS`)\n",
    "- All rows which have a missing value in the feature or the target column are dropped\n",
    "- In case that the dataset has more than 5,000 rows the score is only calculated on a random subset of 5,000 rows with a fixed random seed (`ppscore.RANDOM_SEED`). You can adjust the number of rows or skip this sampling via the API. However, in most scenarios the results will be very similar.\n",
    "- There is no grid search for optimal model parameters\n",
    "\n",
    "We can install by doing <code>pip install ppscore</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "\n",
    "# before using pps, let's drop country and year\n",
    "dfcopy = df.copy()\n",
    "dfcopy.drop(['country', 'year'], axis='columns', inplace=True)\n",
    "\n",
    "#this needs some minor preprocessing because seaborn.heatmap unfortunately does not accept tidy data\n",
    "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbf3abf4",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We gonna skip for this tutorial.  But we can certainly try to combine some columsn to create new features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x is our strong features\n",
    "X = df[        ['income', 'school', 'adult-mort']        ]\n",
    "\n",
    "#y is simply the life expectancy col\n",
    "y = df[\"life-exp\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfea971",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd4215d",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['income', 'school', 'adult-mort']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c17fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['income', 'school', 'adult-mort']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb06f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f861942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c19471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='adult-mort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the training set first!\n",
    "X_train['school'].fillna(X_train['school'].mean(), inplace=True)\n",
    "X_train['income'].fillna(X_train['income'].median(), inplace=True)\n",
    "X_train['adult-mort'].fillna(X_train['adult-mort'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the testing set with the training distribution first!\n",
    "X_test['school'].fillna(X_train['school'].mean(), inplace=True)\n",
    "X_test['income'].fillna(X_train['income'].median(), inplace=True)\n",
    "X_test['adult-mort'].fillna(X_train['adult-mort'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4926526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for y\n",
    "y_train.fillna(y_train.median(), inplace=True)\n",
    "y_test.fillna(y_train.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "X_train[['income', 'school', 'adult-mort']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['income', 'school', 'adult-mort']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum(), y_test.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b80cd03",
   "metadata": {},
   "source": [
    "### Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb13273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of columns.\n",
    "col_dict = {'adult-mort':1,'income':2,'school':3}\n",
    "\n",
    "# Detect outliers in each variable using box plots.\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for variable,i in col_dict.items():\n",
    "                     plt.subplot(5,4,i)\n",
    "                     plt.boxplot(X_train[variable])\n",
    "                     plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d646458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(col, data = X_train):\n",
    "    \n",
    "    # calculate your 25% quatile and 75% quatile\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    \n",
    "    # calculate your inter quatile\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # min_val and max_val\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    \n",
    "    # count number of outliers, which are the data that are less than min_val or more than max_val calculated above\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    \n",
    "    # calculate the percentage of the outliers\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    \n",
    "    if(outlier_count > 0):\n",
    "        print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
    "        print('Number of outliers: {}'.format(outlier_count))\n",
    "        print('Percent of data that is outlier: {}%'.format(outlier_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    outlier_count(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "#x = (x - mean) / std\n",
    "#why do we want to scale our data before data analysis / machine learning\n",
    "\n",
    "#allows your machine learning model to catch the pattern/relationship faster\n",
    "#faster convergence\n",
    "\n",
    "#how many ways to scale\n",
    "#standardardization <====current way\n",
    "# (x - mean) / std\n",
    "#--> when your data follows normal distribution\n",
    "\n",
    "#normalization <---another way\n",
    "# (x - x_min) / (x_max - x_min)\n",
    "#---> when your data DOES NOT follow normal distribution (e.g., audio, signal, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check shapes of all X_train, X_test, y_train, y_test\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "Let's define some algorithms and compare them using cross-validation.\n",
    "\n",
    "[Scikit-Learn](http://scikit-learn.org) provides quick access to a huge pool of machine learning algorithms.\n",
    "\n",
    "Before using sklearn, there is **one thing you need to know**, i.e., the **data shape that sklearn wants**.\n",
    "\n",
    "To apply majority of the algorithms, sklearn requires two inputs, i.e., $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "-  $\\mathbf{X}$, or the **feature matrix** *typically* has the shape of ``[n_samples, n_features]``\n",
    "-  $\\mathbf{y}$, or the **target/label vector** *typically* has the shape of ``[n_samples, ]`` or ``[n_samples, n_targets]`` depending whether that algorithm supports multiple labels\n",
    "\n",
    "Note 1:  if you $\\mathbf{X}$ has only 1 feature, the shape must be ``[n_samples, 1]`` NOT ``[n_samples, ]``\n",
    "\n",
    "Note 2:  sklearn supports both numpy and pandas, as long as the shape is right.  For example, if you use pandas, $\\mathbf{X}$ would be a dataframe, and $\\mathbf{y}$ could be a series or dataframe.\n",
    "\n",
    "Tips:  it's always better to look at sklearn documentation before applying any algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\n",
    "print(\"r2: \", r2_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efd13a",
   "metadata": {},
   "source": [
    "### Much better: Cross validation + Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Libraries for model evaluation\n",
    "\n",
    "# models that we will be using, put them in a list\n",
    "algorithms = [LinearRegression(), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(random_state = 0), \n",
    "              RandomForestRegressor(n_estimators = 100, random_state = 0)]\n",
    "\n",
    "# The names of the models\n",
    "algorithm_names = [\"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some simple cross-validation here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85366d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "#lists for keeping mse\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "#defining splits\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, model in enumerate(algorithms):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {scores.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...it seems random forest do very well....how about we grid search further to find the best version of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'bootstrap': [True], 'max_depth': [5, 10, None],\n",
    "              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfold, \n",
    "                    n_jobs = -1, \n",
    "                    return_train_score=True, \n",
    "                    refit=True,\n",
    "                    scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit your grid_search\n",
    "grid.fit(X_train, y_train);  #fit means start looping all the possible parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your grid_search's best score\n",
    "best_mse = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse  # ignore the minus because it's neg_mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ff42019",
   "metadata": {},
   "source": [
    "## 7. Testing\n",
    "\n",
    "Of course, once we do everything.  We can try to shoot with the final test set.  We should no longer do anything like improving the model.  It's illegal!  since X_test is the final final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = grid.predict(X_test)\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6074023",
   "metadata": {},
   "source": [
    "## 8. Analysis:  Feature Importance\n",
    "\n",
    "Understanding why is **key** to every business, not how low MSE we got.  Extracting which feature is important for prediction can help us interpret the results.  There are several ways: algorithm, permutation, and shap.  Note that these techniques can be mostly applied to most algorithms. \n",
    "\n",
    "Most of the time, we just apply all, and check the consistency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073fb9a1",
   "metadata": {},
   "source": [
    "#### Algorithm way\n",
    "\n",
    "Some ML algorithms provide feature importance score after you fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bed781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stored in this variable\n",
    "#note that grid here is random forest\n",
    "rf = grid.best_estimator_\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot\n",
    "plt.barh(X.columns, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm...let's sort first\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2398c6b",
   "metadata": {},
   "source": [
    "#### Permutation way\n",
    "\n",
    "This method will randomly shuffle each feature and compute the change in the modelâ€™s performance. The features which impact the performance the most are the most important one.\n",
    "\n",
    "*Note*: The permutation based importance is computationally expensive. The permutation based method can have problem with highly-correlated features, it can report them as unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "#let's plot\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d076c453",
   "metadata": {},
   "source": [
    "#### Shap way\n",
    "\n",
    "The SHAP interpretation can be used (it is model-agnostic) to compute the feature importances. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction. It can be easily installed (<code>pip install shap</code>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap provides plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference\n",
    "\n",
    "To provide inference service or deploy, it's best to save the model for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model/life-expectancy.model'\n",
    "pickle.dump(grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09331fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to create one silly example\n",
    "df[['income', 'school', 'adult-mort', 'life-exp']].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c157da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['income', 'school', 'adult-mort'] \n",
    "sample = np.array([[0.476, 10.000, 271.000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_life_exp = loaded_model.predict(sample)\n",
    "predicted_life_exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f16c30a",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Why this dataset is a regression problem?\n",
    "2.  How many samples of data do we have?  How many features do we have?  How many label do we have?\n",
    "3.  Notice \"status\" is a object dtype.  What is \"object\" dtype?\n",
    "4.  Notice \"float64\".  What 64 means?\n",
    "5.  Create another countplot.\n",
    "6.  Create another displot.\n",
    "7.  Create another boxplot.\n",
    "8.  Create another scatterplot.\n",
    "9.  We have used \"label-encoding\" which turns category into 0, 1, 2, ...   Another technique is called one-hot encoding.  What's the difference, and what's the pros and cons?\n",
    "10. What is `random_state` = 42\"\n",
    "11. How do we know whether to replace with mean or median?\n",
    "12. How many ways to scale data, and when to use them?   Try not to scale, and see whether the mse changes.  Why?\n",
    "13. Why we should preprocess only AFTER we split the dataset?\n",
    "14. Try to change to other `X` and try find the best three features which get the best mse.\n",
    "15. How much mse or $r^2$ is considered good or bad?\n",
    "16. How do I know which algorithm to use?  There are so many regression algorithms.\n",
    "17. We use kfold cross-validation.  Try to find other ways of cross-validation.  Hint: search inside sklearn website.\n",
    "18. In cross-validation and grid search, did we touch the testing set?\n",
    "19. Why do we need to do grid search after cross validation?\n",
    "20. What is feature importance?\n",
    "21. After I got the model, then how can I create a website/mobile application?  Explain in details. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
