{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a umbrella term that refers to approaches that help reduce the chance of overfitting, usually either by normalization or through added penatlies.\n",
    "\n",
    "### 1. Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n \\boldsymbol{\\theta}_j^2$$\n",
    "\n",
    "where $\\lambda$ is a free parameter that controls the strength of the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2+ \\lambda\\sum_{j=1}^n |\\theta_j|$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: that is, lasso regression can set certain features to zero, while ridge regression focuses on shrinking all parameters so it is not overly reliant on one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic net \n",
    "\n",
    "When you lack domain knowledge, you can simply use elastic net.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^n |\\theta_j| + (1 - \\lambda) \\sum_{k=1}^n \\theta_k^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso or Elastic net??\n",
    "\n",
    "Regularization should be ALMOST ALWAYS used, since these techniques reduces overfitting.\n",
    "\n",
    "How to choose is a little bit difficult. It is easier to understand the assumptions behind.\n",
    "1.  Ridge assumes that coefficients are normally distributed.   **Thus, if you don't want any feature to dominate too much, use Ridge.**\n",
    "2. Lasso assumes that coefficients are Laplace distributed (in layman sense, it mean some predictors are very useful while some are completely irrelevant).   Here, Lasso has the ability to shrink coefficient to zero thus eliminate predictors that are not useful to the output, thus automatic feature selection.  **In simple words, if you have only very few predictors with medium/large effect, use Lasso.**\n",
    "3.  Elastic basically is a compromise between the two, and thus take huge computation time to reach that compromise.  **If you have the resource to spare, you can use Elastic net**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/218838100027711537', creation_time=1693278210177, experiment_id='218838100027711537', last_update_time=1693278210177, lifecycle_stage='active', name='chaky-regularization-example', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "import os\n",
    "# This the dockerized method.\n",
    "# We build two docker containers, one for python/jupyter and another for mlflow.\n",
    "# The url `mlflow` is resolved into another container within the same composer.\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "# In the dockerized way, the user who runs this code will be `root`.\n",
    "# The MLflow will also log the run user_id as `root`.\n",
    "# To change that, we need to set this environ[\"LOGNAME\"] to your name.\n",
    "os.environ[\"LOGNAME\"] = \"chaky\"\n",
    "# mlflow.create_experiment(name=\"chaky-diabetes-example\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"chaky-regularization-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "print(\"Features: \", diabetes.feature_names)\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    #in this class, we add cross validation as well for some spicy code....\n",
    "    kfold = KFold(n_splits=3)\n",
    "            \n",
    "    def __init__(self, regularization, lr=0.001, method='batch', num_epochs=500, batch_size=50, cv=kfold):\n",
    "        self.lr         = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            \n",
    "        #create a list of kfold scores\n",
    "        self.kfold_scores = list()\n",
    "        \n",
    "        #reset val loss\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        #kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            with mlflow.start_run(run_name=f\"Fold-{fold}\", nested=True):\n",
    "                \n",
    "                params = {\"method\": self.method, \"lr\": self.lr, \"reg\": type(self).__name__}\n",
    "                mlflow.log_params(params=params)\n",
    "                \n",
    "                for epoch in range(self.num_epochs):\n",
    "                \n",
    "                    #with replacement or no replacement\n",
    "                    #with replacement means just randomize\n",
    "                    #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                    #shuffle your index\n",
    "                    perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                            \n",
    "                    X_cross_train = X_cross_train[perm]\n",
    "                    y_cross_train = y_cross_train[perm]\n",
    "                    \n",
    "                    if self.method == 'sto':\n",
    "                        for batch_idx in range(X_cross_train.shape[0]):\n",
    "                            X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                            y_method_train = y_cross_train[batch_idx] \n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    elif self.method == 'mini':\n",
    "                        for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                            #batch_idx = 0, 50, 100, 150\n",
    "                            X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                            y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                            train_loss = self._train(X_method_train, y_method_train)\n",
    "                    else:\n",
    "                        X_method_train = X_cross_train\n",
    "                        y_method_train = y_cross_train\n",
    "                        train_loss = self._train(X_method_train, y_method_train)\n",
    "\n",
    "                    mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
    "\n",
    "                    yhat_val = self.predict(X_cross_val)\n",
    "                    val_loss_new = self.mse(y_cross_val, yhat_val)\n",
    "                    mlflow.log_metric(key=\"val_loss\", value=val_loss_new, step=epoch)\n",
    "                    \n",
    "                    #early stopping\n",
    "                    if np.allclose(val_loss_new, self.val_loss_old):\n",
    "                        break\n",
    "                    self.val_loss_old = val_loss_new\n",
    "            \n",
    "                self.kfold_scores.append(val_loss_new)\n",
    "                print(f\"Fold {fold}: {val_loss_new}\")\n",
    "            \n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        m    = X.shape[0]        \n",
    "        grad = (1/m) * X.T @(yhat - y) + self.regularization.derivation(self.theta)\n",
    "        self.theta = self.theta - self.lr * grad\n",
    "        return self.mse(y, yhat)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercept\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create `Ridge`, `Lasso` and `Elastic` class that extends the `LinearRegression`, with added penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoPenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l # lambda value\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)\n",
    "    \n",
    "class RidgePenalty:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def __call__(self, theta): #__call__ allows us to call class as method\n",
    "        return self.l * np.sum(np.square(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta\n",
    "    \n",
    "class ElasticPenalty:\n",
    "    \n",
    "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
    "        self.l = l \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def __call__(self, theta):  #__call__ allows us to call class as method\n",
    "        l1_contribution = self.l_ratio * self.l * np.sum(np.abs(theta))\n",
    "        l2_contribution = (1 - self.l_ratio) * self.l * 0.5 * np.sum(np.square(theta))\n",
    "        return (l1_contribution + l2_contribution)\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        l1_derivation = self.l * self.l_ratio * np.sign(theta)\n",
    "        l2_derivation = self.l * (1 - self.l_ratio) * theta\n",
    "        return (l1_derivation + l2_derivation)\n",
    "    \n",
    "class Lasso(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = LassoPenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class Ridge(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = RidgePenalty(l)\n",
    "        super().__init__(self.regularization, lr, method)\n",
    "        \n",
    "class ElasticNet(LinearRegression):\n",
    "    \n",
    "    def __init__(self, method, lr, l, l_ratio=0.5):\n",
    "        self.regularization = ElasticPenalty(l, l_ratio)\n",
    "        super().__init__(self.regularization, lr, method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for looping classnames\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Ridge =====\n",
      "Fold 0: 5019.957764397\n",
      "Fold 1: 3789.7283075525406\n",
      "Fold 2: 3231.9871784874213\n",
      "Test MSE:  3952.505584097506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/Github/Machine-Learning/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Lasso =====\n",
      "Fold 0: 3473.727565365972\n",
      "Fold 1: 3417.382642134025\n",
      "Fold 2: 3204.376603720075\n",
      "Test MSE:  2914.256969310874\n",
      "===== ElasticNet =====\n",
      "Fold 0: 3784.5882498239384\n",
      "Fold 1: 3384.868051931774\n",
      "Fold 2: 3064.9560116605444\n",
      "Test MSE:  3065.0966947039674\n"
     ]
    }
   ],
   "source": [
    "regs = [\"Ridge\", \"Lasso\", \"ElasticNet\"]\n",
    "\n",
    "for reg in regs:\n",
    "\n",
    "    params = {\"method\": \"batch\", \"lr\": 0.1, \"l\": 0.1}\n",
    "    mlflow.start_run(run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{reg}\", nested=True)\n",
    "    \n",
    "    print(\"=\"*5, reg, \"=\"*5)\n",
    "\n",
    "    # #######\n",
    "    type_of_regression = str_to_class(reg)    #Ridge, Lasso, ElasticNet\n",
    "    model = type_of_regression(**params)  \n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    mse  = model.mse(y_test, yhat)\n",
    "\n",
    "    print(\"Test MSE: \", mse)\n",
    "    mlflow.log_metric(key=\"test_mse\", value=mse)\n",
    "\n",
    "    signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n",
    "\n",
    "    # #######\n",
    "\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "| | Egg price | Gold price    | Oil price   | GDP   | $\\hat{y}$ |  $(\\hat{y} - y)^2$ |\n",
    "|---:|:------ |:-----------   |:------      |:------|:------    |  :------           |    \n",
    "| 1  | 3      | 100           | 4           | 21    | 28        |  49                |\n",
    "| 2  | 4      | 500           | 7           | 43    | 79        |  1296              |\n",
    "| 3  | 5      | 200           | 6           | 56    | 48        |  64                |\n",
    "| 4  | 6      | 300           | 8           | 21    | 66        |  2025              |\n",
    "| 5  | 7      | 400           | 2           | 44    | 60        |  256               |\n",
    "\n",
    "Note:  this table is filled with given weight of [2, 0.1, 3].\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Explain regularization in your own words.\n",
    "2.  For `LassoPenalty`, show by hand why the derivation is `self.l * np.sign(theta)`\n",
    "3.  For `RidgePenalty`, show by hand why the derivation is `self.l * 2 * theta`\n",
    "4.  Using mini-batch gradient descent based on $(\\mathbf{x}^{(2)}, \\mathbf{x}^{(4)}, \\mathbf{x}^{(5)})$, and using **Ridge regression**, what is $J(\\theta)$?  Assume $\\lambda = 0.1$\n",
    "5.  Continued from above, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$\n",
    "6.  Continued from above, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta}$\n",
    "7.  What would be the new theta?\n",
    "8.  Performed 4,5,6 with **Lasso Regression**.  Assume $\\lambda = 0.1$\n",
    "9.  What does `LinearRegression` inside `Ridge(LinearRegression)` mean?\n",
    "10. What does `str_to_class` function do?\n",
    "11. Based on everything you have done in this workshop, what do you think is a good way to decide when to use Ridge, Lasso, or Elastic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
