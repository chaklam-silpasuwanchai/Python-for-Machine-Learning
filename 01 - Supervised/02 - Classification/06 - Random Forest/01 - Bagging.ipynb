{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree does not perform well as it tends to overfit.  A possible solution is the construct multiple trees to reduce variances.  To make sure each tree is not exactly learning the same thing since it will then be all same trees, we need to inject some differences to these trees (i.e., make them as diverse as possible but at the same time they also see some overlappinp samples).  One simple idea is that each of the tree is trained on a subset of **bootstrapping sample** and then perform some sort of aggregation of the decision.\n",
    "\n",
    "The process has the following steps:\n",
    "\n",
    "1. Sample $m$ times **with replacement** from the original training data\n",
    "2. Repeat $B$ times to generate $B$ \"boostrapped\" training datasets $D_1, D_2, \\cdots, D_B$\n",
    "3. Train $B$ trees using the training datasets $D_1, D_2, \\cdots, D_B$ \n",
    "\n",
    "Boostrapping the data plus performing some sort of aggregation (averaging or majority votes) is called **boostrap aggregation** or **bagging**.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Assume that we have a training set where $m=4$, and $n=2$:\n",
    "\n",
    "$$D = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4)}$$\n",
    "\n",
    "We generate, say, $B = 3$ datasets by boostrapping:\n",
    "\n",
    "$$D_1 = {(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_3, y_3)}$$\n",
    "$$D_2 = {(x_1, y_1), (x_4, y_4), (x_4, y_4), (x_3, y_3)}$$\n",
    "$$D_3 = {(x_1, y_1), (x_1, y_1), (x_2, y_2), (x_2, y_2)}$$\n",
    "\n",
    "We can then train 3 trees.\n",
    "\n",
    "Note: When sampling is performed **without** replacement, it is called **pasting**.  In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Let's try to code from scratch.  To make our life easier, we shall use DecisionTree from the sklearn library (since we already code it from scratch in the previous class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/1h7r513n71j7569x87spb4b40000gn/T/ipykernel_90339/3285919430.py:42: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  yhat = stats.mode(predictions)[0][0] #the first zero gives us the mode array, the second zero convert from 2D ([[]]) to 1D ([])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "B = 5\n",
    "m, n = X_train.shape\n",
    "boostrap_ratio = 1\n",
    "tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(B)]\n",
    "\n",
    "#sample size for each tree\n",
    "sample_size = int(boostrap_ratio * len(X_train))\n",
    "\n",
    "xsamples = np.zeros((B, sample_size, n))\n",
    "ysamples = np.zeros((B, sample_size))\n",
    "\n",
    "#subsamples for each model\n",
    "for i in range(B):\n",
    "    ##sampling with replacement; i.e., sample can occur more than once\n",
    "    #for the same predictor\n",
    "    for j in range(sample_size):\n",
    "        idx = random.randrange(m)   #<----with replacement #change so no repetition\n",
    "        xsamples[i, j, :] = X_train[idx]\n",
    "        ysamples[i, j] = y_train[idx]\n",
    "        #keep track of idx that i did not use for ith tree\n",
    "\n",
    "#fitting each estimator\n",
    "for i, model in enumerate(models):\n",
    "    _X = xsamples[i, :]\n",
    "    _y = ysamples[i, :]\n",
    "    model.fit(_X, _y)\n",
    "    \n",
    "#make prediction and return the probabilities\n",
    "predictions = np.zeros((B, X_test.shape[0]))\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    predictions[i, :] = yhat\n",
    "    \n",
    "#predictions.shape = (B, m_test)\n",
    "    \n",
    "yhat = stats.mode(predictions)[0][0] #the first zero gives us the mode array, the second zero convert from 2D ([[]]) to 1D ([])\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "'''\n",
    "To perform in sklearn, we can use the BaggingClassifier API.  \n",
    "Pasting can be done using BaggingClassifier< setting boostrap=False\n",
    "'''\n",
    "\n",
    "bag = BaggingClassifier(tree, n_estimators=5, max_samples=0.99)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "yhat = bag.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
