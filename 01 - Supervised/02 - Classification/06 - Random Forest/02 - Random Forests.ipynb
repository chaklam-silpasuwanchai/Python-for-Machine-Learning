{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Bag Evaluation\n",
    "\n",
    "Well, it seems like our bagging technique is quite good.  Anyhow, one interesting observation is that each tree only see a subset of the dataset. Any data that a particular tree did not see is called **out of bag** (oob).  Note that oob is not the same for all predictors.\n",
    "\n",
    "One interesting thing is that since oob is something that each tree never see, thus oob is somewhat a validation set.  Thus what we can do is after we fit each tree. We can ask each tree to test their accuracy with their own oob, and then we can average the accuracy from all trees.  \n",
    "\n",
    "Let's modify the above scratch code to:\n",
    "- Calculate for oob evaluation for each bootstrapped dataset, and also the average score\n",
    "- Change the code to \"without replacement\"\n",
    "- Put everything into a class <code>Bagging</code>.  It should have at least two methods, <code>fit(X_train, y_train)</code>, and <code>predict(X_test)</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing, it seems bagging works well.  However, the $B$ bootstrapped dataset are correlated, thus the power of variance reduction is diminished.  How do we further de-correlate these $B$ trees?\n",
    "\n",
    "A **random forest** is constructed by bagging, but for each split in each tree, only a random subset of $q \\leq n$ features are considered as splitting variables.\n",
    "\n",
    "Rule of thumb: $q = \\sqrt{n}$ for classification trees and $q = \\frac{n}{3}$ for regression trees\n",
    "\n",
    "- Modify the code from above to randomize features.  Set the number of features to be used in each tree to be <code>sqrt(n)</code>, and then select a subset of features for each tree.  This can be easily done by setting our DecisionTreeClassifier <code>max_features</code> to 'sqrt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Out of bag score for each tree======\n",
      "Tree 0 0.9047619047619048\n",
      "Tree 1 0.9523809523809523\n",
      "Tree 2 1.0\n",
      "Tree 3 0.9523809523809523\n",
      "Tree 4 0.8571428571428571\n",
      "======Average out of bag score======\n",
      "0.9333333333333332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random, math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, B, bootstrap_ratio, with_no_replacement=True):\n",
    "        self.B = B\n",
    "        self.bootstrap_ratio = bootstrap_ratio\n",
    "        self.with_no_replacement = with_no_replacement\n",
    "        self.tree_params = {'max_depth': 2, 'max_features': 'sqrt'}\n",
    "        self.models = [DecisionTreeClassifier(**self.tree_params) for _ in range(B)]\n",
    "                \n",
    "    def fit(self, X, y):  #<---X_train, y_train\n",
    "        m, n = X.shape\n",
    "\n",
    "        #sample size for each tree\n",
    "        sample_size = int(self.bootstrap_ratio * len(X))\n",
    "\n",
    "        xsamples = np.zeros((self.B, sample_size, n))\n",
    "        ysamples = np.zeros((self.B, sample_size))\n",
    "\n",
    "        xsamples_oob = []  #use list because length is not known\n",
    "        ysamples_oob = []\n",
    "\n",
    "        #bootstrapping samples for each model\n",
    "        for i in range(self.B):\n",
    "            oob_idx = []\n",
    "            idxes = []\n",
    "            for j in range(sample_size):\n",
    "                idx = random.randrange(m)\n",
    "                if (self.with_no_replacement):\n",
    "                    while idx in idxes:\n",
    "                        idx = random.randrange(m)\n",
    "                idxes.append(idx)\n",
    "                oob_idx.append(idx)\n",
    "                xsamples[i, j, :] = X[idx]\n",
    "                ysamples[i, j] = y[idx]\n",
    "            mask = np.zeros((m), dtype=bool)\n",
    "            mask[oob_idx] = True\n",
    "            xsamples_oob.append(X[~mask])\n",
    "            ysamples_oob.append(y[~mask])\n",
    "    \n",
    "        #fitting each estimator\n",
    "        oob_score = 0\n",
    "        print(\"======Out of bag score for each tree======\")\n",
    "        for i, model in enumerate(self.models):\n",
    "            \n",
    "            _X = xsamples[i]\n",
    "            _y = ysamples[i]\n",
    "            model.fit(_X, _y)\n",
    "\n",
    "            #calculating oob score\n",
    "            _X_test = np.asarray(xsamples_oob[i])\n",
    "            _y_test = np.asarray(ysamples_oob[i])\n",
    "            yhat = model.predict(_X_test)\n",
    "            oob_score += accuracy_score(_y_test, yhat)\n",
    "            print(f\"Tree {i}\", accuracy_score(_y_test, yhat))\n",
    "        self.avg_oob_score = oob_score / len(self.models)\n",
    "        print(\"======Average out of bag score======\")\n",
    "        print(self.avg_oob_score)\n",
    "    \n",
    "    def predict(self, X): #<---X_test\n",
    "        #make prediction and return the probabilities\n",
    "        predictions = np.zeros((self.B, X.shape[0]))\n",
    "        for i, model in enumerate(self.models):\n",
    "            yhat = model.predict(X)\n",
    "            predictions[i, :] = yhat\n",
    "        return stats.mode(predictions)[0][0]\n",
    "\n",
    "model = RandomForest(B=5, bootstrap_ratio=0.8)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 2, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the same as RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"n_estimators\": [10, 50, 100], \n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": np.arange(1, 10)}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, refit=True)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "yhat = grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Random Forests\n",
    "\n",
    "Advantages of Random Forest:\n",
    "\n",
    "- Voting helps overcome overfitting\n",
    "- Because bagging and pasting support parallel computing (e.g., using <code>n_jobs</code>), they are very popular methods.\n",
    "- Random forest can solve both type of problems that is classification and regression and does a decent estimation at both fronts.\n",
    "- The power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method. Further, the model outputs importance of variable, which can be a very handy feature.  Sklearn implements <code>feature_importances_</code> in <code>RandomForestClassifier</code> which helps you understand which feature is useful for classification in Random Forest\n",
    "- It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing (I did not really touch this, but I recommend you to check it out)\n",
    "- It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "- The capability of the above can be extended to unlabeled data, leading to unsupervised clustering,data views and outlier detection.\n",
    "- Just like other ensemble, it works well with structured/tabular data.  Indeed, XGBoost (another ensemble method) is among the best classifier for structured/tabular data and often used for Kaggle competition.  But if we are working with image, sound, brain signal, deep learning remains the way to go.\n",
    "- Unlike Decision Trees, multiple trees can give out probability\n",
    "- Out of bag evaluation is handy\n",
    "\n",
    "Disadvantages of Random Forest:\n",
    "\n",
    "- It surely does a good job at classification but not as for regression problem as it does not gives precise continuous nature prediction. In case of regression, it doesn't predict beyond the range in the training data, and that they may over fit data sets that are particularly noisy.\n",
    "- Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds.\n",
    "- At one point, more samples will not improve the accuracy, unlike deep neural network\n",
    "- It fails when there are rare outcomes or rare predictors, as the algorithm is based on bootstrap sampling. This makes it non-ideal if you're working with rare personality traits, high segmented customer behavior, or rare variants in genomics research.\n",
    "\n",
    "In conclusion, if you are working with structured/tabular data, and would like high accuracy but does not care much about interpretability (just like most Kaggle competition does), you may want to use ensemble methods (including Random Forests and the like)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
