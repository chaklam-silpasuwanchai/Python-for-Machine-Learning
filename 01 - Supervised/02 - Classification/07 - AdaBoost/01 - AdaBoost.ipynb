{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost** is a boosting algorithm that try to take a weak classifier on top of one another, **boosting** the overall performance. AdaBoost is extremely simple to use and implement, and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Anyhow, Decision Tree with <code>max_depth=1</code> and <code>max_leaf_nodes=2</code> are often used (also known as **stump**)\n",
    "\n",
    "<img src = \"figures/ada2.png\" />\n",
    "\n",
    "## Hypothesis function\n",
    "\n",
    "Suppose we are given training data ${(\\mathbf{x}^{(i)}, y^{(i)})}$, where $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\{-1, 1\\}$.  And we have $S$ number of weak classifiers, denoted $h_s(\\mathbf{x}^{(i)})$.  For each classifier, we define $\\alpha_s$ as the *voting power* of the classifier $h_s(\\mathbf{x}^{(i)})$.  Higher the alpha, the more we trust that classifier. Then, the hypothesis function is based on a linear combination of the weak classifier and is written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(\\mathbf{x}^{(i)}) & = \\text{sign}\\big(\\alpha_1h_1(\\mathbf{x}^{(i)}) + \\alpha_2h_2(\\mathbf{x}^{(i)}) + \\cdots + \\alpha_sh_s(\\mathbf{x}^{(i)}) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(\\mathbf{x}^{(i)})\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Define what is a good classifier\n",
    "\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more).  To get this alpha, we should first define what is \"good\" classifier.  This is simple, since good classifier should simply has the minimum weighted errors as:\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{range}(\\epsilon_s) = [0, 1]$$\n",
    "\n",
    "in which the weights are initialized in the beginning as\n",
    "\n",
    "$$w_s^{(i)} = \\frac{1}{m}$$ \n",
    "\n",
    "where\n",
    "\n",
    "$$\\sum_{i=1}^m w_s^{(i)} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given $h(\\mathbf{x})$ as <code>yhat</code> and <code>y</code> as the real y, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = np.array([-1,  1, -1, 1, 1]) #(h_s(x))\n",
    "y    = np.array([ 1,  1,  1, 1, 1])\n",
    "(yhat != y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate its weighted errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "m = 5 #since we have five samples\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "w = np.full(m, 1/m)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "e = w[(yhat != y)].sum() / sum(w)\n",
    "print(e.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to change our weight bigger for the first one, you will see that the final error is enlarged.  (Please don't mind why it became 0.7 or 0.05; this is just example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.7, 0.05, 0.05, 0.1, 0.1]) / sum(w)\n",
    "e = w[(yhat != y)].sum()\n",
    "print(e.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights\n",
    "\n",
    "Our goal is that once we got the error, we need to emphasize the incorrectly classified sample, so the next classifier will focus on making them right.  Thus we need a weight update rule.  The formula is as follows:\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then need to renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\\sum_{i=1}^m w_s^{(i)} = 1$$\n",
    "\n",
    "Here $\\alpha_s$ is:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{range}(\\alpha_s) = (-\\infty, \\infty)$$\n",
    "\n",
    "\n",
    "## Relationship between alpha and errors\n",
    "\n",
    "Here, higher the error, lower is alpha, which means we don't trust that classifier.  And vice versa.   If e is close to 0 (the classifier performs well), alpha will be positive, indicating that the classifier's predictions should have more influence on the final ensemble. If e is close to 0.5 (the classifier performs worst than random guessing), alpha will be negative, indicating that the classifier's predictions should have less influence, and they will be \"flipped\" in the ensemble.   Of course, if e is 1, then we should NOT even use this classifier!!\n",
    "\n",
    "First, to see why this formula works, let's plot alpha against errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Alpha')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e = np.linspace(0, 0.99, 10000) #select only 0.99 not 1 because np.log(1-1=0) = inf\n",
    "e = e + 0.00001 #prevent divide by zero\n",
    "a_j = np.log ((1 - e) / e) * 0.5  #np.log is ln by default\n",
    "\n",
    "plt.plot(e, a_j)\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Alpha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this weight update rule works?\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "Let's first find the alpha.  Recall that:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1] [-1  1 -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#recall the y and yhat\n",
    "print(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "#recall the error\n",
    "w = np.full(m, 1/m)\n",
    "e = w[(yhat != y)].sum() / sum(w)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2027325540540821\n"
     ]
    }
   ],
   "source": [
    "#calculate the a_j\n",
    "a_j = np.log ((1 - e) / e) * 0.5\n",
    "print(a_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we find the initial alpha, let's plug everthing into the weight update rule, starting from this component:\n",
    "\n",
    "$$h_s(\\mathbf{x^{(i)}}) y^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand what multiplying actually means.  Notice that negative means the answer is wrong.\n",
    "\n",
    "We multiply negative alpha so that incorrectly classified sample will have bigger value.\n",
    "\n",
    "Why do we need to make the incorrectly classified sample have bigger value?  Well, because we want incorrectly classified sample to have bigger weight so the next classifier can focus on it.\n",
    "\n",
    "$$-\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20273255, -0.20273255,  0.20273255, -0.20273255, -0.20273255])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-a_j * yhat * y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since we will multiply this with the weight, we perform exp to make sure that the resulting number is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22474487, 0.81649658, 1.22474487, 0.81649658, 0.81649658])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-a_j * yhat * y)  \n",
    "\n",
    "#we perform np.exp to keep them positive number because weight cannot be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we calculate everything.\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then need to renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24494897, 0.16329932, 0.24494897, 0.16329932, 0.16329932])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w * np.exp(-a_j * yhat * y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.16666667, 0.25      , 0.16666667, 0.16666667])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize\n",
    "w = w / sum(w)  \n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what does this number means?**\n",
    "\n",
    "Well, notice that the incorrectly classified samples are #1 and #3, and they both have bigger weights than others.   This will make sure next classifier will focus on solving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative alpha?\n",
    "\n",
    "Notice that when alpha is negative, wrongly classified samples do not get bigger weights.  Why?\n",
    "\n",
    "In short, by assigning negative alpha values, the algorithm effectively flips the predictions of the weak classifier, so their combined effect is closer to random guessing, reducing their influence on the final strong classifier.\n",
    "\n",
    "In some cases, we should just throw all classifiers having negative alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "1. Create your first classifier (e.g., `DecisionTrees`) and fit the training set.\n",
    "\n",
    "2. Calculate the error of the first classifier\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$\n",
    "\n",
    "3. Calculate alpha of the first classifier\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "4. Exaggerate the incorrect samples using\n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$\n",
    "\n",
    "5. Repeat 1.\n",
    "\n",
    "6. We stop 1-4 using early stopping, reached number of classifiers, or certain error threshold is reached.\n",
    "\n",
    "7. To predict, we use the hypothesis function:\n",
    "\n",
    "$$ \n",
    "  h(\\mathbf{x}) = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(\\mathbf{x})\\big)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_0: 1.4573816055101643\n",
      "a_1: 0.4861280111506205\n",
      "a_2: 0.2585364906779028\n",
      "a_3: 0.5638365346080928\n",
      "a_4: 0.47162902784400756\n",
      "a_5: 0.28504868984885795\n",
      "a_6: 0.23151576673325486\n",
      "a_7: 0.3518645068326187\n",
      "a_8: 0.3605190018519204\n",
      "a_9: 0.3147970623243156\n",
      "a_10: 0.3702580922616173\n",
      "a_11: 0.38906768724141066\n",
      "a_12: 0.38029109425895147\n",
      "a_13: 0.30624491550448657\n",
      "a_14: 0.4736826102203567\n",
      "a_15: 0.38344932944148\n",
      "a_16: 0.3951664184073484\n",
      "a_17: 0.4072703833775688\n",
      "a_18: 0.2930361975822689\n",
      "a_19: 0.40154165674893993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "#initially, we set our weight to 1/m\n",
    "w = np.full(m, 1/m)\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = w)\n",
    "    \n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train) \n",
    "    \n",
    "    e = w[(yhat != y_train)].sum() / sum(w)\n",
    "    \n",
    "    # print(f\"e_{j}: {e}\")\n",
    "        \n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    a_j = np.log ((1 - e) / e) / 2\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    print(f\"a_{j}: {a_j}\")\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    w = (w * np.exp(-a_j * y_train * yhat))\n",
    "    w = w / sum(w)    \n",
    "    \n",
    "    # print(sum(w))  #make sure always sum to 1\n",
    "\n",
    "        \n",
    "#make weighted predictions\n",
    "h = 0\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    h += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(h)\n",
    "\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains an AdaBoost classifier based on 200 Decision stumps.  A Decision stump is basically a Decision Tree with max_depth=1.  This is the default base estimator of AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group workshop\n",
    "\n",
    "Given `yhat = [-1, 1, -1, 1, 1]` and `y = [1, 1, 1, 1, 1]`:\n",
    "\n",
    "1. Calculate the weights of each sample using \n",
    "\n",
    "$$w_s^{(i)} = \\frac{1}{m}$$\n",
    "\n",
    "2. What does $s$ here mean? What does these weights even mean?  \n",
    "3. What is the sum of all weights?  What is the range of each weight?\n",
    "4. Calculate its weighted error using\n",
    "\n",
    "$$\\epsilon_s = \\frac{\\sum_{i=1}^m w_s^{(i)}I(h_s(\\mathbf{x}^{(i)}) \\neq y^{(i)})}{\\sum_{i=1}^m w_s^{(i)}}$$\n",
    "\n",
    "5. Okay, we got a number called $\\epsilon$.  What this $\\epsilon$ means??\n",
    "6. Calculate alpha using \n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}\\ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "7. Okay, we got a number called $\\alpha$.  What this $\\alpha$ means?  Higher means what? Lower?\n",
    "8. Calculate \n",
    "\n",
    "$$e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}} $$\n",
    "\n",
    "9. What does these numbers mean?  \n",
    "10. Finally, calculate \n",
    "\n",
    "$$w_{s+1}^{(i)} = w_s^{(i)}e^{ -\\alpha_sh_s(\\mathbf{x^{(i)}}) y^{(i)}}$$\n",
    "\n",
    "which then renormalize \n",
    "\n",
    "$$w_{s+1}^{(i)} = \\frac{w_{s+1}^{(i)}}{{\\displaystyle\\sum_{i=1}^m w_{s+1}^{(i)}}} $$\n",
    "\n",
    "11. We got some new weights.  What these new weights trying to achieve?\n",
    "    \n",
    "12. So in AdaBoost, when do we stop?  List three ways.\n",
    "    \n",
    "13. Take a look at `Extended - AdaBoost.ipynb` for additional details - if you are interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
