{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.    \n",
    "\n",
    "## Hypothesis function\n",
    "\n",
    "The hypothesis function of gradient boosting is as follows:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}^{(i)}) = h_0(\\mathbf{x}^{(i)}) + \\alpha_1h_1(\\mathbf{x}^{(i)}) + \\cdots + \\alpha_s h_s(\\mathbf{x}^{(i)})\n",
    "$$\n",
    "\n",
    "Similar to AdaBoost, decision trees are typically used for each $h_s(\\mathbf{x}^{(i)})$.\n",
    "\n",
    "However, although they look similar, notice that \n",
    "1. No alpha is applied to the first predictor, because the learning is \"sequential\"\n",
    "2. In addition, all alpha shares the same number.  Here, alpha is like the learning rate in regression.\n",
    "\n",
    "You will clearly understand these differences as we go.\n",
    " \n",
    "To simplify, we shall first talk about gradient boosting, assuming a regression problem.  We shall later move to classification which is trivial.\n",
    "\n",
    "## Gradient Boosting for Regression\n",
    "\n",
    "Firstly, let's look at the following equation where $h_0(\\mathbf{x}^{(i)})$ is our first predictor and we would like to minimize the residual as follows:\n",
    "\n",
    "$$h_0(\\mathbf{x}^{(i)}) + \\text{residual}_0 = y^{(i)} $$\n",
    "$$ \\text{residual}_0 =  y^{(i)} - h_0(\\mathbf{x}^{(i)}) $$\n",
    "\n",
    "That is, we would $y$ to be as close as $h$ such that residual is 0\n",
    "\n",
    "$$ y^{(i)} = h_0(\\mathbf{x}^{(i)}); \\text{s.t.  residual} = 0$$\n",
    "\n",
    "The question is that is it possible to add the second predictor $h_1(\\mathbf{x}^{(i)})$ such that the residual is further reduced\n",
    "\n",
    "$$ y^{(i)} = h_0(\\mathbf{x}^{(i)}) + h_1(\\mathbf{x}^{(i)}) $$\n",
    "\n",
    "This equation can be written as:\n",
    "\n",
    "$$h_1(\\mathbf{x}^{(i)}) = y^{(i)} - h_0(\\mathbf{x}^{(i)}) $$\n",
    "\n",
    "This equation informs us that if we can find a subsequent predictor that can best fit the \"residual\" (i.e. $y^{(i)} - h_0(\\mathbf{x}^{(i)})$), then we can improve the accuracy.\n",
    "\n",
    "## Proof\n",
    "\n",
    "Well, firstly, here is our loss function for regression:\n",
    "\n",
    "$$J = \\frac{1}{2}(y^{(i)} - h(\\mathbf{x}^{(i)}))^2$$\n",
    "\n",
    "In linear regression, we will find $\\mathbf{w}$ that can minimize $J$ by finding the gradient of $J$ in respect to $\\mathbf{w}$.\n",
    "\n",
    "But in gradient boosting, we want to find $h$ that can minimize $J$.  Thus, we need to find the gradient of $J$ in respect to $h(\\mathbf{x})$.   The equation can be written as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h(\\mathbf{x}^{(i)})} = h(\\mathbf{x}^{(i)}) - y^{(i)} $$\n",
    "\n",
    "Based on gradient descent, we take the negative gradients for the update which is $y - h_0(x)$\n",
    "\n",
    "$$h_1(\\mathbf{x}^{(i)}) = - \\frac{\\partial J}{\\partial h_0(\\mathbf{x}^{(i)})} = -(h_0(\\mathbf{x}^{(i)}) - y^{(i)}) = y^{(i)} - h_0(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "or more generally (where $s$ is the index of predictor)\n",
    "\n",
    "$$h_s(\\mathbf{x}^{(i)}) = - \\frac{\\partial J}{\\partial h_{s-1}(\\mathbf{x}^{(i)})} = y^{(i)} - h_{s-1}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "\n",
    "## Classification?\n",
    "\n",
    "In cross entropy, the loss function is\n",
    "\n",
    " $$J= y \\lg h(\\mathbf{x}) + (1 - y) \\lg (1-h(\\mathbf{x}))$$\n",
    " \n",
    "The derivative of $J$ in respect to $h(\\mathbf{x})$ will be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(\\mathbf{x})} = h(\\mathbf{x}) - y$$\n",
    "\n",
    "This may look the same as mse, but note that our $h(\\mathbf{x})$ (i.e., regression tree) outputs continuous values.  In order to transform $h(x)$ into discrete class, we shall transform using sigmoid function $g$ as follows:\n",
    "\n",
    "$$g(h(\\mathbf{x})) = g(\\mathbf{z}) = \\frac{1}{1+e^{-\\mathbf{z}}}$$\n",
    "\n",
    "For multiclass classification, $g$ is defined as the softmax function:\n",
    "\n",
    "$$g(h(\\mathbf{x})) = g(\\mathbf{z}) = \\frac{e^\\mathbf{z}_c}{\\displaystyle\\sum_{i=1}^{k} e^\\mathbf{z}_k}$$\n",
    "\n",
    "Also remind that to use softmax function, we need to first one-hot encode our y.  And during prediction, we need to perform <code>np.argmax</code> along the `axis=1`.\n",
    "\n",
    "## Adding learning rate\n",
    "\n",
    "To make sure adding the subsequent predictor would not overfit our model, we shall add an learning rate $\\alpha$ in front of this, which shall be the same across all predictors (different from AdaBoost where alpha is different across all predictors)\n",
    "\n",
    "$$h_s(\\mathbf{x}) = - \\alpha \\frac{\\partial J}{\\partial h_{s-1}(\\mathbf{x})}$$\n",
    "\n",
    "## How does it look for the third predictors?\n",
    "\n",
    "$$ \\text{residual}_1 =  y - (h_0(\\mathbf{x}) + \\alpha h_1(\\mathbf{x}))$$\n",
    "\n",
    "then we define $h_2(\\mathbf{x})$ as \n",
    "\n",
    "$$h_2(\\mathbf{x}) = \\alpha(y - (h_0(\\mathbf{x}) + \\alpha h_1(\\mathbf{x})))$$\n",
    "\n",
    "And then repeat\n",
    "\n",
    "The final prediction shall use the following hypothesis function:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}^{(i)}) = h_0(\\mathbf{x}^{(i)}) + \\alpha_1h_1(\\mathbf{x}^{(i)}) + \\cdots + \\alpha_s h_s(\\mathbf{x}^{(i)})\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is simply a fixed learning rate, same across all $h$.\n",
    "\n",
    "## When do we stop?\n",
    "\n",
    "1. When we reached desired iterations\n",
    "2. When the residual does not decrease further using some validation set\n",
    "\n",
    "## Summary of steps\n",
    "\n",
    "1. Initialize the model as simply mean or some constant\n",
    "2. Predict and calculate the residual\n",
    "3. Let the next model fit the residual\n",
    "4. Predict using the combined models and calculate the residual\n",
    "5. Let the next model fit this residual\n",
    "6. Simply repeat 4-5 until stopping criteria is reached"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = grad(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use our scratch code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MSE:  2714.188989170066\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Our MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn \n",
    "\n",
    "sklearn has implemented GradientBoosting under the API of <code>GradientBoostingClassifier</code> for classification and <code>GradientBoostingRegressor</code> for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE:  2715.7388118438184\n"
     ]
    }
   ],
   "source": [
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting, designed to be more efficient, flexible, and portable (Chen and Guestrin 2016).  In fact, XGBoost is often an important component of the winning entries in ML competitions (e.g., Kaggle).  XGBoost also offers several nice features, such as automatically taking care of early stopping: XGBoost’s API is quite similar to Scikit-Learn’s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:64.95543\n",
      "[1]\tvalidation_0-rmse:59.66044\n",
      "[2]\tvalidation_0-rmse:57.59860\n",
      "[3]\tvalidation_0-rmse:56.58414\n",
      "[4]\tvalidation_0-rmse:56.67145\n",
      "[5]\tvalidation_0-rmse:56.75270\n",
      "MSE: 3201.7650905241712\n"
     ]
    }
   ],
   "source": [
    "#make sure to pip install xgboost\n",
    "#for mac guys, do \"brew install libomp\" which installs openMP library\n",
    "#required for XGBoost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(early_stopping_rounds=2) \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)])\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "Let's summarize some useful info about Gradient Boosting:\n",
    "\n",
    "Advantages:\n",
    "1. Extremely powerful - especially useful for heterogeneous data (e.g., house price, number of bedrooms). \n",
    "\n",
    "Disadvantages:\n",
    "1. They cannot be parallelized.  Obvious since they are sequential predictors.\n",
    "2. They can easily overfit, thus require careful choice of estimators or the use of regularization such as max_depth.\n",
    "3. When we talk about homogeneous data such as images, videos, audio, text, or huge amount of data, deep learning works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop\n",
    "\n",
    "1. What are some observable differences between AdaBoost and Gradient Boosting?\n",
    "2. What is the main idea of Gradient Boosting?\n",
    "3. Why do we find the gradient of $J$ in respect to $h$?\n",
    "4. Why Chaky say that ``finding the next $h$ is simply creating a model to fit the residuals\"?\n",
    "5. For Gradient Boosting, we got some $\\alpha$ to set?  What is the range of this value?  Is it same across all models?\n",
    "6. Why do we need learning rate?\n",
    "7. In Gradient Boosting, why the first model does not have alpha?\n",
    "8. In Gradient Boosting, when do we stop?\n",
    "9. In the code, it was written `DummyRegressor(strategy='mean')`.  What is it?    For classification, what will be a good first model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
