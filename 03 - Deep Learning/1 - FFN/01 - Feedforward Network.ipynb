{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104d26190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "#We can check whether we have gpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have linear regression as a case study to study the different components of PyTorch.  These are the following components we will be covering:\n",
    "\n",
    "1. Specifying input and target\n",
    "2. Dataset and DataLoader\n",
    "3. `nn.Linear` (Dense)\n",
    "4. Define loss function\n",
    "5. Define optimizer function\n",
    "6. Train the model\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\" width=\"400\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Specifiying input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "x_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(x_train)\n",
    "targets = torch.from_numpy(y_train)\n",
    "print(inputs.size())\n",
    "print(targets.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader\n",
    "\n",
    "We'll create a <code>TensorDataset</code>, which allows access to rows from inputs and targets as tuples, and if we want to use <code>DataLoader</code> (will talk shortly) from numpy array, we have to first make <code>TensorDataset</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a <code>DataLoader</code>, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 3\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader is typically used in a for-in loop. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[102.,  43.,  37.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[ 22.,  37.],\n",
      "        [119., 133.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data, with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, which can lead to faster reduction in the loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define some layer - nn.Linear\n",
    "\n",
    "Instead of initializing the weights & biases manually, we can define the model using the <code>nn.Linear</code> class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1265,  0.1165, -0.2811],\n",
      "        [ 0.3391,  0.5090, -0.4236]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([0.5018, 0.1081], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define model\n",
    "model = nn.Linear(3, 2)  #nn.Linear assume this shape (in_features, out_features)\n",
    "print(model.weight)\n",
    "print(model.weight.size()) # (out_features, in_features)\n",
    "print(model.bias)\n",
    "print(model.bias.size()) #(out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our model is simply a function that performs a matrix multiplication of the <code>inputs</code> and the weights <code>w</code> and adds the bias <code>b</code> (for each observation)\n",
    "\n",
    "<img src = \"figures/dot.png\" width=\"400\">\n",
    "\n",
    "PyTorch models also have a helpful <code>.parameters</code> method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1265,  0.1165, -0.2811],\n",
       "         [ 0.3391,  0.5090, -0.4236]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5018, 0.1081], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())  #model.param returns a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the <code>model(tensor)</code> API to perform a forward-pass that generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-13.0136,  40.7472],\n",
       "        [-18.7468,  48.6438],\n",
       "        [-11.1951,  73.2410],\n",
       "        [-17.7916,  40.9065],\n",
       "        [-16.7183,  42.7146],\n",
       "        [-13.0136,  40.7472],\n",
       "        [-18.7468,  48.6438],\n",
       "        [-11.1951,  73.2410],\n",
       "        [-17.7916,  40.9065],\n",
       "        [-16.7183,  42.7146],\n",
       "        [-13.0136,  40.7472],\n",
       "        [-18.7468,  48.6438],\n",
       "        [-11.1951,  73.2410],\n",
       "        [-17.7916,  40.9065],\n",
       "        [-16.7183,  42.7146]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define loss function\n",
    "\n",
    "The <code>nn</code> module contains a lot of useful loss function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_mse = nn.MSELoss()\n",
    "criterion_softmax_cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6058.1660, grad_fn=<MseLossBackward0>)\n",
      "6058.166015625\n"
     ]
    }
   ],
   "source": [
    "mse = criterion_mse(preds, targets)\n",
    "print(mse)\n",
    "print(mse.item())  ##print out the loss number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimizer\n",
    "\n",
    "We use <code>optim.SGD</code> to perform stochastic gradient descent where samples are selected in batches (often with random shuffling) instead of as a single group.  Note that <code>model.parameters()</code> is passed as an argument to <code>optim.SGD</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "#momentum update the weight based on past gradients also, which will be useful for getting out of local max/min\n",
    "#If our momentum parameter was $0.9$, we would get our current grad + the multiplication of the gradient \n",
    "#from one time step ago by $0.9$, the one from two time steps ago by $0.9^2 = 0.81$, etc.\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training - putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            xb.to(device) #move them to gpu if possible, if not, it will be cpu\n",
    "            yb.to(device)\n",
    "                    \n",
    "            # 1. Predict\n",
    "            pred = model(xb)\n",
    "                      \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Calculate gradient\n",
    "            opt.zero_grad()  #if not, the gradients will accumulate\n",
    "            loss.backward()\n",
    "            \n",
    "            # Print out the gradients.\n",
    "            #print ('dL/dw: ', model.weight.grad) \n",
    "            #print ('dL/db: ', model.bias.grad)\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            sys.stdout.write(\"\\rEpoch [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 15.8259"
     ]
    }
   ],
   "source": [
    "#train for 100 epochs\n",
    "fit(100, model, criterion_mse, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.768131256103516\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "loss = criterion_mse(preds, targets)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop\n",
    "\n",
    "1. What does `torch.manual_seed(42)` do?  Why 42?\n",
    "2. What is `TensorDataset`?\n",
    "3. What is the purpose of `DataLoader`?\n",
    "4. What does `batch_size` in `DataLoader` means?\n",
    "5. What does `shuffle` in `DataLoader` means?\n",
    "6. Given $\\mathbf{X}$ of shape (4000, 6), i.e., 4000 samples with 6 features, create a `nn.Linear` layer that projects it to 8 features.\n",
    "7. How do we access the `weight` of the `nn.Linear` layer?\n",
    "8. When we type `model.parameters()`, we see `requires_grad=True`, what does that means?\n",
    "9. When we print `preds`, we saw `grad_fn=<AddmmBackward0`, what does that mean?\n",
    "10. What is the difference between `mse` and `mse.item()`, according to the above code?\n",
    "11. What is the role of `torch.optim.SGD`?\n",
    "12. What does `xb.to(device)` means?\n",
    "13. Finally, you will realize this is almost identical to `Linear Regression` you have previously studied.  Try to wrap your head around and think about this.\n",
    "14. How do you think, comparing `nn.Linear` vs. `GradientBoosting`?  Try to search internet or ask ChatGPT about this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
